{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial \n",
    "\n",
    "## Part-III\n",
    "\n",
    "In this live session we will **(a)** reorganize the input as tokenized into analyses, **(b)** parse each analysis into its root and a stream of affixes (another level of tokenization), **(c)** parse the stream into separate morphemes (yet another), **(d)** extract a set of morphemes and **(e)** export the results in *csv* format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"./in/\"\n",
    "output_path = \"./out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** read input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_store_input(input_path):\n",
    "    # initialize dictionary\n",
    "    data = {}\n",
    "    # read file names from the input directory\n",
    "    input_files = glob.glob(input_path+'*.txt')\n",
    "    for file_path in input_files:\n",
    "        content = open(file_path, 'r')\n",
    "        # create an empty list with a key as the name of the current file\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        data[file_name]=[]\n",
    "        # populate the list with content from file\n",
    "        for line in content:\n",
    "            # check if line has content\n",
    "            if line.strip() != \"\":\n",
    "                data[file_name].append(line)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_and_store_input(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## database\n",
    "\n",
    "**current**: \n",
    "```\n",
    "    {   \n",
    "        file_1:[line_1, ..., line_n],\n",
    "        ...,\n",
    "        file_m:[line_1, ..., line_k]\n",
    "    }\n",
    "```\n",
    "\n",
    "**envisioned**:\n",
    "```\n",
    "    {\n",
    "        file_name: {\n",
    "                        word_index: {\n",
    "                                        analysis_index: {\n",
    "                                                            'root'  :root,\n",
    "                                                            'aff'   :[affixes],\n",
    "                                                            'pos'   :pos_tag,\n",
    "                                                            'proper':bool\n",
    "                                                        }\n",
    "                                    }\n",
    "                    }      \n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a-1) tokenize lines into streams of analyses \n",
    "note that each stream corresponds to a word in the original data; associated **delimiter** is a `single (white)space`, or, `' '`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_l2w(data):\n",
    "    # 'l' for line, 'w' for words\n",
    "    # traverse data\n",
    "    for file, content in data.items():\n",
    "        word_index = 0\n",
    "        words = {}\n",
    "        for line in content:\n",
    "            # split() = split(' ')\n",
    "            for word in line.split():\n",
    "                words[word_index] = word\n",
    "                word_index += 1\n",
    "        # make changes in situ, i.e. update\n",
    "        data[file] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_l2w(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a-2) tokenize streams into individual analyses\n",
    "associated **delimiter** is a `forward slash`, or, `'/'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_w2a(data):\n",
    "    # 'w' for word, 'a' for analyses\n",
    "    # traverse data\n",
    "    for file, words in data.items():\n",
    "        for word_index, word in words.items():\n",
    "            analyses = {}\n",
    "            for index, analysis in enumerate(word.split('/')):\n",
    "                analyses[index] = analysis\n",
    "            # make changes in situ, i.e. update\n",
    "            data[file][word_index] = analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_w2a(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in data.items():\n",
    "    print(values[4][1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b-1) tokenize each analysis into a tuple: (morp, is_proper)\n",
    "associated **delimiter** is a `complex custom string`, in this case, `'+[Proper='`. for why, please refer to the manual of the morphological analyzer used in tutorial part II ([here](https://github.com/google-research/turkish-morphology))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_a2t(data):\n",
    "    # 'a' for analysis, 't' for tuple\n",
    "    # traverse data\n",
    "    for file, words in data.items():\n",
    "        for word_index, word in words.items():\n",
    "            for analysis_index, analysis in word.items():\n",
    "                morphology, is_proper = analysis.split('+[Proper=')\n",
    "                is_proper = is_proper.strip(']')\n",
    "                if is_proper == 'False':\n",
    "                    is_proper = False\n",
    "                elif is_proper == 'True':\n",
    "                    is_proper = True\n",
    "                else:\n",
    "                    raise ValueError('unexpected value for var=is_proper')\n",
    "                tup = (morphology, is_proper)\n",
    "                # make changes in situ, i.e. update\n",
    "                data[file][word_index][analysis_index] = tup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_a2t(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in data.items():\n",
    "    print(values[4][1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b-2, c) tokenize morphology into roots, affixes and PoS tags\n",
    "associated **delimiters** are `plus`, `minus` and `forward square bracket` characters, or, `'+'`, `'-'` and `'['` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_values():\n",
    "    values = {\n",
    "                'root':None,\n",
    "                'affixes':None,\n",
    "                'pos':None,\n",
    "                'is_proper':None\n",
    "            }\n",
    "    return values\n",
    "\n",
    "def tokenizer_m2c(data):\n",
    "    # 'm' for morphology, 'c' for category\n",
    "    # traverse data\n",
    "    for file, words in data.items():\n",
    "        for word_index, word in words.items():\n",
    "            for analysis_index, analysis in word.items():\n",
    "                morphology, is_proper = analysis\n",
    "                # remove parantheses from morph. analysis\n",
    "                # this will remove grouping information, ref. manual\n",
    "                morphology = ''.join(ch for ch in morphology if ch not in '()')\n",
    "                # split morphology\n",
    "                parts = [part for chunk in morphology.split('+') for part in chunk.split('-')]\n",
    "                root_and_pos = parts[0]\n",
    "                root = root_and_pos.split('[')[0]\n",
    "                pos = root_and_pos.split('[')[1].strip(']')\n",
    "                # control if the analysis yielded any affixes\n",
    "                if len(parts) > 1:\n",
    "                    affixes = parts[1:]\n",
    "                else:\n",
    "                    affixes = None\n",
    "                #fill in values\n",
    "                values = init_values()\n",
    "                values['root'] = root\n",
    "                values['affixes'] = affixes\n",
    "                values['pos'] = pos\n",
    "                values['is_proper'] = is_proper\n",
    "                # make changes in situ, i.e. update\n",
    "                data[file][word_index][analysis_index] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_m2c(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in data.items():\n",
    "    pprint(values[4][1])\n",
    "    print('\\n')\n",
    "    pprint(values[4][1]['affixes'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) extract a set of roots and affixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catalog(data):\n",
    "    cat = {'roots':[], 'affixes':[]}\n",
    "    for file, words in data.items():\n",
    "        for w_index, word in words.items():\n",
    "            for a_index, analysis in word.items():\n",
    "                root = analysis['root']\n",
    "                if root not in cat['roots']:\n",
    "                    cat['roots'].append(root)\n",
    "                affixes = analysis['affixes']\n",
    "                # check if morph. analysis yielded any affixes\n",
    "                if affixes:\n",
    "                    for affix in affixes:\n",
    "                        if affix not in cat['affixes']:\n",
    "                            cat['affixes'].append(affix)\n",
    "    return cat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = catalog(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e-1) export the data in *csv* format\n",
    "let the fields be `'file_name'`, `'word_index'`, `'analysis_index'`, `'root'`, `'pos'`, `'is_prop'` and `morpheme_types`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['file_index', 'word_index', 'analysis_index', 'root', 'pos', 'is_prop'] + ['afx:'+affix for affix in cat['affixes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(data, fields, affix_cats, output_path):\n",
    "    csv = open(output_path+'data.csv','w')\n",
    "    for file_name, content in data.items():\n",
    "        f_index = file_name.split('_')[0]\n",
    "        # init file\n",
    "        fields_line = ','.join(field for field in fields)\n",
    "        csv.write(fields_line+'\\n')\n",
    "        for w_index, word in content.items():\n",
    "            for a_index, analysis in word.items():\n",
    "                root = analysis['root']\n",
    "                pos = analysis['pos']\n",
    "                is_proper = str(int(analysis['is_proper']))\n",
    "                affixes = analysis['affixes']\n",
    "                if not affixes:\n",
    "                    affixes = []\n",
    "                csv.write(f_index+',')\n",
    "                csv.write(str(w_index)+',')\n",
    "                csv.write(str(a_index)+',')\n",
    "                csv.write(root+',')\n",
    "                csv.write(pos+',')\n",
    "                csv.write(is_proper+',')\n",
    "                for cat in affix_cats:\n",
    "                    if cat in affixes:\n",
    "                        csv.write('1,')\n",
    "                    else:\n",
    "                        csv.write('0,')\n",
    "                csv.write('\\n')\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(data, fields, cat['affixes'], output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e-2) export the catalog into two files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_catalogs(catalog, output_path):\n",
    "    \n",
    "    root_cat = open(output_path+'root_catalog.txt','w')\n",
    "    for root in catalog['roots']:\n",
    "        root_cat.write(root+'\\n')\n",
    "    root_cat.close()\n",
    "    \n",
    "    afx_cat = open(output_path+'affix_catalog.txt','w')\n",
    "    for afx in catalog['affixes']:\n",
    "        afx_cat.write(afx+'\\n')\n",
    "    afx_cat.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_catalogs(cat, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}