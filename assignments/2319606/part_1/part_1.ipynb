{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export first 10019 tokens from metu turkish corpus\n",
    "\n",
    "with open(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\in.txt\",encoding = \"utf8\") as file:\n",
    "    file1 = file.readlines()\n",
    "file2 = file1[:10019]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing sentences from tokens\n",
    "def constructsentence(input_file,listofsentences=[],temp=\"\"):\n",
    "    for e in input_file:\n",
    "        if e[:-1] == \"<s>\":\n",
    "            temp = \"\"\n",
    "        elif e[:-1] == \"</s>\":\n",
    "            listofsentences.append(temp)\n",
    "        else:\n",
    "            temp += e[:-1]+\" \"\n",
    "    return listofsentences\n",
    "sentence_data = constructsentence(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koca bir duvar taşıyordun yüreğinde kimsenin aşamayacağı , aşmaya cesaret bile edemeyeceği . \n"
     ]
    }
   ],
   "source": [
    "print(sentence_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating second input data which has one sentence in every line.\n",
    "\n",
    "file = open(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\in2.txt\",\"w\",encoding = \"utf8\")\n",
    "items = sentence_data\n",
    "items = map(lambda x: x + '\\n', items)\n",
    "for e in items:\n",
    "    file.write(e)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spelling corrected results after apply itü nlp spell corrector to the sentence data. \n",
    "\n",
    "with open(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\spellingcorrected.txt\",encoding = \"utf8\") as file3:\n",
    "    file4 = file3.readlines()\n",
    "finaldata = [elem[:-2] for elem in file4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halel , demiştin , neler yaptı ?\n",
      "Halil , demiştin , neler yaptı ?\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(finaldata)):\n",
    "    if finaldata[i] != sentence_data[i][:-1]:\n",
    "        print(finaldata[i])\n",
    "        print(sentence_data[i][:-1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning from numbers, dates, punctuations. Since my list keeps sentences discretely, i will delete sentence splitters too.\n",
    "\n",
    "def cleansentence(input_file,cleanedsentences=[]):\n",
    "    for i in range(len(input_file)):\n",
    "        e,res = input_file[i],\"\"\n",
    "        for token in e.split():\n",
    "            temp = \"\"\n",
    "            for char in token:\n",
    "                if char not in string.punctuation and not char.isnumeric():\n",
    "                    temp += char\n",
    "            if temp != \"\":\n",
    "                res += temp+\" \"\n",
    "        cleanedsentences.append(res[:-1])\n",
    "    return cleanedsentences\n",
    "cleaned_data = cleansentence(finaldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koca bir duvar taşıyordun yüreğinde kimsenin aşamayacağı aşmaya cesaret bile edemeyeceği\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case normalization PROPN excluded (without using POS tagging or NER): We have some procedures here:\n",
    "#first: If the first letter is uppercase and the rest is lowercase, do not normalize.\n",
    "#Second: If all letters are uppercase, it is abbreviation and might be PROPN. Do not normalize.\n",
    "#Third: For all other cases should be accepted as typo and should be normalized.\n",
    "\n",
    "#Addition Note: In this function, some non-PROPN tokens will not be normalized(some first tokens for sentences).\n",
    "#If we use a hash table to keep the tokens which is normalized in the sentences(except to first tokens), we can \n",
    "#detect PROPNs at the beggining of the sentence with this hash table. However, this kind of generalization needs a big amount of data.\n",
    "\n",
    "def case_normalize(input_data,result=[]):\n",
    "    for sentence in input_data:\n",
    "        uptemp = \"\"\n",
    "        for e in sentence.split():\n",
    "            if len(e)>1: #we assume that minimum length of a proper noun is 2.\n",
    "                flag,cnt = e[0].isupper(),len(e)-1\n",
    "                for f in e[1:]:\n",
    "                    if not flag:\n",
    "                        break\n",
    "                    elif f.isupper():\n",
    "                        cnt -= 1\n",
    "                temp = \"\"\n",
    "                if not flag or (cnt>0 and cnt<len(e)-1):\n",
    "                    for g in e:\n",
    "                        temp += g.lower()\n",
    "                else:\n",
    "                    for g in e:\n",
    "                        temp += g\n",
    "                uptemp += temp+\" \"\n",
    "            else:\n",
    "                uptemp += e[0].lower()+\" \"\n",
    "        result.append(uptemp[:-1])\n",
    "    return result\n",
    "case_normalized_data = case_normalize(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O kadar çok şey hatırlıyorum ki\n",
      "o kadar çok şey hatırlıyorum ki\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(case_normalized_data)):\n",
    "    if cleaned_data[i] != case_normalized_data[i]:\n",
    "        print(cleaned_data[i])\n",
    "        print(case_normalized_data[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\out\\\\out.txt\",\"w\",encoding = \"utf8\")\n",
    "items = case_normalized_data\n",
    "items = map(lambda x: x + '\\n', items)\n",
    "for e in items:\n",
    "    file.write(e)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
