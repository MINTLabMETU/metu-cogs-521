{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-) Functions for reading and writing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_path):\n",
    "    with open(input_path,encoding = \"utf8\") as file:\n",
    "        result = file.readlines()\n",
    "    return result\n",
    "\n",
    "def write_output(output_path,items):\n",
    "    file = open(output_path,\"w\",encoding = \"utf8\")\n",
    "    items = map(lambda x: x + '\\n', items)\n",
    "    for e in items:\n",
    "        file.write(e)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-) Constructing sentences from tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructsentence(input_file,listofsentences=[],temp=\"\"):\n",
    "    for e in input_file:\n",
    "        if e[:-1] == \"<s>\":\n",
    "            temp = \"\"\n",
    "        elif e[:-1] == \"</s>\":\n",
    "            listofsentences.append(temp)\n",
    "        else:\n",
    "            temp += e[:-1]+\" \"\n",
    "    return listofsentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-) Cleaning numbers, dates, punctuations. Since my list keeps sentences discretely, i will delete sentence splitters too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansentence(input_file,cleanedsentences=[]):\n",
    "    for i in range(len(input_file)):\n",
    "        e,res = input_file[i],\"\"\n",
    "        for token in e.split():\n",
    "            temp = \"\"\n",
    "            for char in token:\n",
    "                if char not in string.punctuation and not char.isnumeric():\n",
    "                    temp += char\n",
    "            if temp != \"\":\n",
    "                res += temp+\" \"\n",
    "        cleanedsentences.append(res[:-1])\n",
    "    return cleanedsentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-) Case normalization PROPN excluded (without using POS tagging or NER): We have some procedures here:\n",
    "\n",
    "first: If the first letter is uppercase and the rest is lowercase, do not normalize.\n",
    "Second: If all letters are uppercase, it is abbreviation and might be PROPN. Do not normalize.\n",
    "Third: For all other cases should be accepted as typo and should be normalized.\n",
    "\n",
    "Addition Note: In this function, some non-PROPN tokens will not be normalized(some first tokens for sentences).\n",
    "If we use a hash table to keep the tokens which is normalized in the sentences(except to first tokens), we can \n",
    "detect PROPNs at the beggining of the sentence with this hash table. However, this kind of generalization needs a big amount of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_normalize(input_data,result=[]):\n",
    "    for sentence in input_data:\n",
    "        uptemp = \"\"\n",
    "        for e in sentence.split():\n",
    "            if len(e)>1: #we assume that minimum length of a proper noun is 2.\n",
    "                flag,cnt = e[0].isupper(),len(e)-1\n",
    "                for f in e[1:]:\n",
    "                    if not flag:\n",
    "                        break\n",
    "                    elif f.isupper():\n",
    "                        cnt -= 1\n",
    "                temp = \"\"\n",
    "                if not flag or (cnt>0 and cnt<len(e)-1):\n",
    "                    for g in e:\n",
    "                        temp += g.lower()\n",
    "                else:\n",
    "                    for g in e:\n",
    "                        temp += g\n",
    "                uptemp += temp+\" \"\n",
    "            else:\n",
    "                uptemp += e[0].lower()+\" \"\n",
    "        result.append(uptemp[:-1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export first 10019 tokens from metu turkish corpus and creating second input data which has one sentence in every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koca bir duvar taşıyordun yüreğinde kimsenin aşamayacağı , aşmaya cesaret bile edemeyeceği . \n"
     ]
    }
   ],
   "source": [
    "file1 = read_input(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\in.txt\")\n",
    "file2 = file1[:10019]\n",
    "\n",
    "sentence_data = constructsentence(file2)\n",
    "print(sentence_data[0])\n",
    "\n",
    "file = write_output(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\in2.txt\",sentence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import spelling corrected results after apply itü nlp spell corrector to the sentence data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halel , demiştin , neler yaptı ?\n",
      "Halil , demiştin , neler yaptı ?\n"
     ]
    }
   ],
   "source": [
    "file4 = read_input(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\in\\\\spellingcorrected.txt\")\n",
    "finaldata = [elem[:-2] for elem in file4]\n",
    "\n",
    "for i in range(len(finaldata)):\n",
    "    if finaldata[i] != sentence_data[i][:-1]:\n",
    "        print(finaldata[i])\n",
    "        print(sentence_data[i][:-1])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example data point after cleaning the numbers and punctuations (since dates are \"DD+Punct.+MM+Punct.+YYYY\" format, they will be cleaned automatically.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koca bir duvar taşıyordun yüreğinde kimsenin aşamayacağı aşmaya cesaret bile edemeyeceği\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = cleansentence(finaldata)\n",
    "print(cleaned_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example data point after case normalization. Then, write the results to the \"out.txt\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O kadar çok şey hatırlıyorum ki\n",
      "o kadar çok şey hatırlıyorum ki\n"
     ]
    }
   ],
   "source": [
    "case_normalized_data = case_normalize(cleaned_data)\n",
    "for i in range(len(case_normalized_data)):\n",
    "    if cleaned_data[i] != case_normalized_data[i]:\n",
    "        print(cleaned_data[i])\n",
    "        print(case_normalized_data[i])\n",
    "        break\n",
    "    \n",
    "file = write_output(\"C:\\\\Users\\\\erolc\\\\Desktop\\\\cogs520\\\\part_1\\\\out\\\\out.txt\",case_normalized_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
